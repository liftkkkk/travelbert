{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitanaconda3virtualenv464894319aa646bba76abd2d1bc9681c",
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/lvzhiheng/Desktop/data/travel_KG\"\n",
    "baike_dir = os.path.join(data_dir, \"带url的正文和摘要triple/第二轮\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#entity with fine-grained types: 1830\n#total entities: 75049\n"
     ]
    }
   ],
   "source": [
    "coarse_types = [\"菜品\", \"建筑\", \"景点\", \"老字号门店\", \"人物\", \"文物\", \"组织机构\"]\n",
    "\n",
    "# convert type names to full path names, e.g. \"名人故居\" -> \"/景点/人文历史景点/名人故居\"\n",
    "parent_class = {}\n",
    "with open(os.path.join(data_dir, \"subclassof.txt\"), \"r\") as file:\n",
    "    for line in file:\n",
    "        category, parent_category = line.strip().split()\n",
    "        parent_class[category] = parent_category\n",
    "\n",
    "# query up parent_class until root\n",
    "full_type_name = {}\n",
    "for category in parent_class:\n",
    "    full_name = category\n",
    "    parent_category = parent_class[category]\n",
    "    while parent_category != \"\":\n",
    "        full_name = f\"{parent_category}/{full_name}\"\n",
    "        parent_category = parent_class[parent_category] if parent_category in parent_class else \"\"\n",
    "    full_type_name[category] = \"/\" + full_name\n",
    "\n",
    "full_type_name.update({type_name: f\"/{type_name}\" for type_name in coarse_types})\n",
    "\n",
    "# for type_name, full_name in full_type_name.items():\n",
    "#     print(type_name, \"->\", full_name)\n",
    "\n",
    "# read entity types from files\n",
    "entity_types = defaultdict(list)\n",
    "with open(os.path.join(data_dir, \"instanceOf_jd.txt\"), \"r\") as file:\n",
    "    for line in file:\n",
    "        entity, type_name = line.strip().split()\n",
    "        type_name = full_type_name[type_name]\n",
    "        if type_name not in entity_types[entity]:\n",
    "            entity_types[entity].append(type_name)\n",
    "print(f\"#entity with fine-grained types: {len(entity_types)}\")\n",
    "\n",
    "for type_name in coarse_types:\n",
    "    file_path = os.path.join(data_dir, f\"entities/{type_name}.txt\")\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            entity = line.strip()\n",
    "            full_name = full_type_name[type_name]\n",
    "            if full_name in entity_types[entity]:\n",
    "                continue\n",
    "            entity_types[entity].append(full_name)\n",
    "print(f\"#total entities: {len(entity_types)}\")\n",
    "\n",
    "# add all up-level types to entity_types\n",
    "for entity, types in entity_types.items():\n",
    "    expand_types = []\n",
    "    for type_name in types:\n",
    "        levels = type_name[1:].split(\"/\")\n",
    "        for idx in range(1, len(levels) + 1):\n",
    "            parent_type = \"/\" + \"/\".join(levels[:idx])\n",
    "            if parent_type not in expand_types:\n",
    "                expand_types.append(parent_type)\n",
    "    entity_types[entity] = expand_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fet_test_file = os.path.join(data_dir, \"test.json\")\n",
    "\n",
    "with open(fet_test_file, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "test_entities = set()\n",
    "test_examples = []\n",
    "for example in test_data:\n",
    "    sentence = example[\"sent\"]\n",
    "    # replace label with its full name\n",
    "    labels = [\"老字号门店\" if label == \"门店\" else label for label in example[\"labels\"]]\n",
    "    labels = [full_type_name[label] for label in labels]\n",
    "    example[\"labels\"] = labels\n",
    "\n",
    "    entity = sentence[example[\"start\"]:example[\"end\"]]\n",
    "    test_entities.add(entity)\n",
    "    if entity in entity_types and all(label in entity_types[entity] for label in labels):\n",
    "        example[\"labels\"] = entity_types[entity]\n",
    "    test_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_sentences(doc):\n",
    "    content = re.sub(r\"([。？！])\", r\"\\1\\n\", doc)\n",
    "    sentences = []\n",
    "    for sentence in content.split():\n",
    "        if len(sentence) < 4:\n",
    "            continue\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "examples = []\n",
    "# DS to build FET dataset\n",
    "for file_name in os.listdir(baike_dir):\n",
    "    file_path = os.path.join(baike_dir, file_name)\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            # name  (full name in baike)  url  abstract/article\n",
    "            # (full name) / abstract / article may be absent\n",
    "            parts = line.split(\"\\t\\t\")\n",
    "            article = parts[-1].strip()\n",
    "            if len(article) == 0:\n",
    "                continue\n",
    "\n",
    "            paragraphs = article.split(\"::;\")\n",
    "            if paragraphs[0] not in [\"AbstractHere\", \"ArticleHere\"]:\n",
    "                continue\n",
    "            for paragraph in paragraphs[1:]:\n",
    "                if paragraph.startswith(\"==\") and paragraph.endswith(\"==\"):\n",
    "                    continue\n",
    "\n",
    "                for sentence in extract_sentences(paragraph):\n",
    "                    # find anchor links\n",
    "                    anchor_spans = [match.span() for match in re.finditer(r\"\\[\\[(.*?)\\|(.*?)\\]\\]\", sentence)]\n",
    "                    if len(anchor_spans) == 0:\n",
    "                        continue\n",
    "                    text, entity_spans = [], []\n",
    "                    end_point, length = 0, 0\n",
    "                    for start, end in anchor_spans:\n",
    "                        text.append(sentence[end_point:start])\n",
    "                        length += start - end_point\n",
    "                        # replace anchor link with raw text\n",
    "                        entity, url = sentence[start:end][2:-2].split(\"|\")\n",
    "                        text.append(entity)\n",
    "                        entity_spans.append([length, length + len(entity)])\n",
    "                        length += len(entity)\n",
    "                        end_point = end\n",
    "                    text.append(sentence[end_point:])\n",
    "                    text = \"\".join(text)\n",
    "\n",
    "                    for start, end in entity_spans:\n",
    "                        entity = text[start:end]\n",
    "                        # TODO: more accurate ways to get entity types\n",
    "                        if entity not in entity_types or entity in test_entities:\n",
    "                            continue\n",
    "                        examples.append(OrderedDict([\n",
    "                            (\"sent\", text),\n",
    "                            (\"labels\", entity_types[entity]),\n",
    "                            (\"start\", start),\n",
    "                            (\"end\", end)\n",
    "                        ]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#train_examples: 168484, ratio: 0.9000502152846779\n#dev_examples: 18710, ratio: 0.09994978471532208\n"
     ]
    }
   ],
   "source": [
    "train_file = os.path.join(data_dir, \"FET/train.json\")\n",
    "dev_file = os.path.join(data_dir, \"FET/dev.json\")\n",
    "test_file = os.path.join(data_dir, \"FET/test.json\")\n",
    "\n",
    "\n",
    "def remove_duplication(examples):\n",
    "    example_ids = set()\n",
    "    non_dup_examples = []\n",
    "    for example in examples:\n",
    "        # same (sent, start, end) as the same example\n",
    "        example_id = (example[\"sent\"], example[\"start\"], example[\"end\"])\n",
    "        if example_id in example_ids:\n",
    "            continue\n",
    "        non_dup_examples.append(example)\n",
    "        example_ids.add(example_id)\n",
    "    return non_dup_examples\n",
    "\n",
    "\n",
    "examples = remove_duplication(examples)\n",
    "\n",
    "# random split to train & dev\n",
    "entity_examples = defaultdict(list)\n",
    "for idx, example in enumerate(examples):\n",
    "    entity = example[\"sent\"][example[\"start\"] : example[\"end\"]]\n",
    "    entity_examples[entity].append(idx)\n",
    "\n",
    "all_entity = list(entity_examples.keys())\n",
    "random.seed(1234)\n",
    "random.shuffle(all_entity)\n",
    "\n",
    "# ensure train dev no entity overlap\n",
    "# TODO: better ways to compare entites (e.g. entity id) rather than text\n",
    "train_examples, dev_examples = [], []\n",
    "n_entity = 0\n",
    "while n_entity < len(all_entity) and len(train_examples) < len(examples) * 0.9:\n",
    "    entity = all_entity[n_entity]\n",
    "    train_examples += [examples[idx] for idx in entity_examples[entity]]\n",
    "    n_entity += 1\n",
    "\n",
    "while n_entity < len(all_entity):\n",
    "    entity = all_entity[n_entity]\n",
    "    dev_examples += [examples[idx] for idx in entity_examples[entity]]\n",
    "    n_entity += 1\n",
    "print(f\"#train_examples: {len(train_examples)}, ratio: {len(train_examples) / len(examples)}\")\n",
    "print(f\"#dev_examples: {len(dev_examples)}, ratio: {len(dev_examples) / len(examples)}\")\n",
    "\n",
    "\n",
    "test_examples = remove_duplication(test_examples)\n",
    "# train dataset has no \"/景点/自然风光/池塘\" label\n",
    "test_examples = [example for example in test_examples if \"/景点/自然风光/池塘\" not in example[\"labels\"]]\n",
    "\n",
    "for target_file, split_examples in zip([train_file, dev_file, test_file],\n",
    "                                       [train_examples, dev_examples, test_examples]):\n",
    "    with open(target_file, \"w\") as writer:\n",
    "        json.dump(split_examples, writer, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# collect entity url from baike anchor text\n",
    "entity_url = {}\n",
    "for file_name in os.listdir(baike_dir):\n",
    "    file_path = os.path.join(baike_dir, file_name)\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            for match in re.finditer(r\"\\[\\[(.*?)\\|(http.*?)\\]\\]\", line):\n",
    "                entity, url = match.group(1), match.group(2)\n",
    "                entity_url[entity] = url"
   ]
  },
  {
   "source": [
    "## Dataset statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentence length percentage\n",
      "90 114.0\n",
      "91 119.0\n",
      "92 125.0\n",
      "93 133.0\n",
      "94 143.0\n",
      "95 154.0\n",
      "96 171.0\n",
      "97 195.0\n",
      "98 227.0\n",
      "99 293.0\n",
      "100 1838.0\n",
      "#examples in dataset: 168484\n",
      "#labels in dataset: 33\n",
      "#fine grained examples (> 1 labels) ratio: 0.017894874290733838\n",
      "sentence length percentage\n",
      "90 108.0\n",
      "91 113.0\n",
      "92 119.0\n",
      "93 125.0\n",
      "94 134.0\n",
      "95 145.0\n",
      "96 159.0\n",
      "97 182.0\n",
      "98 214.0\n",
      "99 263.0\n",
      "100 963.0\n",
      "#examples in dataset: 18710\n",
      "#labels in dataset: 26\n",
      "#fine grained examples (> 1 labels) ratio: 0.012292891501870658\n",
      "sentence length percentage\n",
      "90 101.0\n",
      "91 105.0\n",
      "92 107.0\n",
      "93 111.0\n",
      "94 114.0\n",
      "95 118.0\n",
      "96 124.96000000000004\n",
      "97 128.0\n",
      "98 133.0\n",
      "99 140.0\n",
      "100 150.0\n",
      "#examples in dataset: 4677\n",
      "#labels in dataset: 33\n",
      "#fine grained examples (> 1 labels) ratio: 0.26726534103057514\n",
      "ratio of entities having > 1 types: 0.03016695758770936\n"
     ]
    }
   ],
   "source": [
    "# train data sentence length distribution\n",
    "import numpy as np\n",
    "\n",
    "def dataset_stat(examples):\n",
    "    print(\"sentence length percentage\")\n",
    "    sent_lens = np.array([len(example[\"sent\"]) for example in examples])\n",
    "    for i in range(90, 101):\n",
    "        print(i, np.percentile(sent_lens, i))\n",
    "\n",
    "    label_set = set()\n",
    "    n_fine_grained = 0\n",
    "    for example in examples:\n",
    "        label_set.update(example[\"labels\"])\n",
    "        if len(example[\"labels\"]) > 1:\n",
    "            n_fine_grained += 1\n",
    "\n",
    "    print(f\"#examples in dataset: {len(examples)}\")\n",
    "    print(f\"#labels in dataset: {len(label_set)}\")\n",
    "    print(f\"#fine grained examples (> 1 labels) ratio: {n_fine_grained / len(examples)}\")\n",
    "\n",
    "\n",
    "dataset_stat(train_examples)\n",
    "dataset_stat(dev_examples)\n",
    "dataset_stat(test_examples)\n",
    "n_fine_grained_entity = sum(1 if len(types) > 1 else 0 for entity, types in entity_types.items())\n",
    "print(f\"ratio of entities having > 1 types: {n_fine_grained_entity / len(entity_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: DS entity disambigution\n",
    "# train dev & test label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check baike text coverage of fine-grained entities\n",
    "fine_ents = set(entity for entity, types in entity_types.items() if len(types) > 1)\n",
    "baike_ents = set()\n",
    "\n",
    "for file_name in os.listdir(baike_dir):\n",
    "    file_path = os.path.join(baike_dir, file_name)\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            name = line.split(\"\\t\\t\")[0]\n",
    "            baike_ents.add(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2264, 1229)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "len(fine_ents), len(baike_ents & fine_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}