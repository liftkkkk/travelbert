{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, OrderedDict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/lvzhiheng/Desktop/data/travel_KG\"\n",
    "baike_dir = os.path.join(data_dir, \"带url的正文和摘要triple/第二轮\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#entity with fine-grained types: 1539\n#total entities: 62073\n"
     ]
    }
   ],
   "source": [
    "coarse_types = [\"建筑\", \"景点\", \"老字号门店\", \"人物\", \"文物\", \"组织机构\"]\n",
    "# types that cannot be identified from text\n",
    "non_expressive_types = [\"文化节庆场所\", \"亲子游景区\"]\n",
    "\n",
    "# convert type names to full path names, e.g. \"名人故居\" -> \"/景点/人文历史景点/名人故居\"\n",
    "parent_class = {}\n",
    "with open(os.path.join(data_dir, \"subclassof.txt\"), \"r\") as file:\n",
    "    for line in file:\n",
    "        category, parent_category = line.strip().split()\n",
    "        parent_class[category] = parent_category\n",
    "\n",
    "# query up parent_class until root\n",
    "full_type_name = {}\n",
    "for category in parent_class:\n",
    "    full_name = category\n",
    "    parent_category = parent_class[category]\n",
    "    while parent_category != \"\":\n",
    "        full_name = f\"{parent_category}/{full_name}\"\n",
    "        parent_category = parent_class[parent_category] if parent_category in parent_class else \"\"\n",
    "    full_type_name[category] = \"/\" + full_name\n",
    "\n",
    "full_type_name.update({type_name: f\"/{type_name}\" for type_name in coarse_types})\n",
    "\n",
    "# for type_name, full_name in full_type_name.items():\n",
    "#     print(type_name, \"->\", full_name)\n",
    "\n",
    "# read entity types from files\n",
    "entity_types = defaultdict(list)\n",
    "with open(os.path.join(data_dir, \"instanceOf_jd.txt\"), \"r\") as file:\n",
    "    for line in file:\n",
    "        entity, type_name = line.strip().split()\n",
    "        # NOTE filter non_expressive_types compared with v1.x\n",
    "        if type_name in non_expressive_types:\n",
    "            continue\n",
    "        type_name = full_type_name[type_name]\n",
    "        if type_name not in entity_types[entity]:\n",
    "            entity_types[entity].append(type_name)\n",
    "print(f\"#entity with fine-grained types: {len(entity_types)}\")\n",
    "\n",
    "for type_name in coarse_types:\n",
    "    file_path = os.path.join(data_dir, f\"entities/{type_name}.txt\")\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            entity = line.strip()\n",
    "            full_name = full_type_name[type_name]\n",
    "            if full_name in entity_types[entity]:\n",
    "                continue\n",
    "            entity_types[entity].append(full_name)\n",
    "print(f\"#total entities: {len(entity_types)}\")\n",
    "\n",
    "# add all up-level types to entity_types\n",
    "for entity, types in entity_types.items():\n",
    "    expand_types = []\n",
    "    for type_name in types:\n",
    "        levels = type_name[1:].split(\"/\")\n",
    "        for idx in range(1, len(levels) + 1):\n",
    "            parent_type = \"/\" + \"/\".join(levels[:idx])\n",
    "            if parent_type not in expand_types:\n",
    "                expand_types.append(parent_type)\n",
    "    entity_types[entity] = expand_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE revise test examples manually\n",
    "prev_test_file = os.path.join(data_dir, \"FET_v1.0/test.json\")\n",
    "\n",
    "with open(prev_test_file, \"r\") as file:\n",
    "    prev_test_examples = json.load(file)\n",
    "single_label_examples = [example for example in prev_test_examples if len(example[\"labels\"]) == 1]\n",
    "\n",
    "# multi_label examples are manually revised\n",
    "with open(os.path.join(data_dir, \"FET/multi_label_split1_tag.json\"), \"r\") as reader:\n",
    "    multi_label_split1 = json.load(reader)\n",
    "with open(os.path.join(data_dir, \"FET/multi_label_split2_tag.json\"), \"r\") as reader:\n",
    "    multi_label_split2 = json.load(reader)\n",
    "multi_label_examples = multi_label_split1 + multi_label_split2\n",
    "\n",
    "# remove empty examples\n",
    "multi_label_examples = [example for example in multi_label_examples if len(example[\"labels\"]) > 0]\n",
    "\n",
    "# remove \"文化节庆场所\", \"亲子游景区\" types\n",
    "for example in multi_label_examples:\n",
    "    filtered_labels = [label for label in example[\"labels\"] if \"文化节庆场所\" not in label and \"亲子游景区\" not in label]\n",
    "    example[\"labels\"] = filtered_labels\n",
    "\n",
    "# test_entities for avoiding train & test entity overlapping\n",
    "test_examples = single_label_examples + multi_label_examples\n",
    "test_entities = set()\n",
    "for example in test_examples:\n",
    "    sentence = example[\"sent\"]\n",
    "    entity = sentence[example[\"start\"]:example[\"end\"]]\n",
    "    test_entities.add(entity)\n",
    "    # sanity check\n",
    "    assert len(example[\"labels\"]) > 0\n",
    "    assert all(\"文化节庆场所\" not in label and \"亲子游景区\" not in label for label in example[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/new_组织机构_bd_abstract.txt...\n",
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/new_组织机构_bd_article.txt...\n",
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/no_article_baike.txt...\n",
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/人物_bd_article.txt...\n",
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/new_建筑_bd_abstract.txt...\n",
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/new_建筑_bd_article.txt...\n",
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/景点_1414_newFile_bd_article.txt...\n",
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/人物_bd_abstract.txt...\n",
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/景点_1414_newFile_bd_abstract.txt...\n",
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/new_文物_bd_article.txt...\n",
      "Read /Users/lvzhiheng/Desktop/data/travel_KG/带url的正文和摘要triple/第二轮/new_文物_bd_abstract.txt...\n"
     ]
    }
   ],
   "source": [
    "def extract_sentences(doc):\n",
    "    content = re.sub(r\"([。？！])\", r\"\\1\\n\", doc)\n",
    "    sentences = []\n",
    "    for sentence in content.split():\n",
    "        if len(sentence) < 4:\n",
    "            continue\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "examples = []\n",
    "# DS to build FET dataset\n",
    "for file_name in os.listdir(baike_dir):\n",
    "    file_path = os.path.join(baike_dir, file_name)\n",
    "    print(f\"Read {file_path}...\")\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            # name  (full name in baike)  url  abstract/article\n",
    "            # (full name) / abstract / article may be absent\n",
    "            parts = line.split(\"\\t\\t\")\n",
    "            article = parts[-1].strip()\n",
    "            if len(article) == 0:\n",
    "                continue\n",
    "\n",
    "            paragraphs = article.split(\"::;\")\n",
    "            if paragraphs[0] not in [\"AbstractHere\", \"ArticleHere\"]:\n",
    "                continue\n",
    "            for paragraph in paragraphs[1:]:\n",
    "                if paragraph.startswith(\"==\") and paragraph.endswith(\"==\"):\n",
    "                    continue\n",
    "\n",
    "                for sentence in extract_sentences(paragraph):\n",
    "                    # find anchor links\n",
    "                    anchor_spans = [match.span() for match in re.finditer(r\"\\[\\[(.*?)\\|(.*?)\\]\\]\", sentence)]\n",
    "                    if len(anchor_spans) == 0:\n",
    "                        continue\n",
    "                    text, entity_spans = [], []\n",
    "                    end_point, length = 0, 0\n",
    "                    for start, end in anchor_spans:\n",
    "                        text.append(sentence[end_point:start])\n",
    "                        length += start - end_point\n",
    "                        # replace anchor link with raw text\n",
    "                        entity, url = sentence[start:end][2:-2].split(\"|\")\n",
    "                        text.append(entity)\n",
    "                        entity_spans.append([length, length + len(entity)])\n",
    "                        length += len(entity)\n",
    "                        end_point = end\n",
    "                    text.append(sentence[end_point:])\n",
    "                    text = \"\".join(text)\n",
    "\n",
    "                    for start, end in entity_spans:\n",
    "                        entity = text[start:end]\n",
    "                        # TODO: more accurate ways to get entity types\n",
    "                        if entity not in entity_types or entity in test_entities:\n",
    "                            continue\n",
    "                        examples.append(OrderedDict([\n",
    "                            (\"sent\", text),\n",
    "                            (\"labels\", entity_types[entity]),\n",
    "                            (\"start\", start),\n",
    "                            (\"end\", end)\n",
    "                        ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#train_examples: 168679, ratio: 0.9002166766288106\n#dev_examples: 18697, ratio: 0.09978332337118948\n"
     ]
    }
   ],
   "source": [
    "train_file = os.path.join(data_dir, \"FET/train.json\")\n",
    "dev_file = os.path.join(data_dir, \"FET/dev.json\")\n",
    "test_file = os.path.join(data_dir, \"FET/test.json\")\n",
    "\n",
    "\n",
    "def remove_duplication(examples):\n",
    "    example_ids = set()\n",
    "    non_dup_examples = []\n",
    "    for example in examples:\n",
    "        # same (sent, start, end) as the same example\n",
    "        example_id = (example[\"sent\"], example[\"start\"], example[\"end\"])\n",
    "        if example_id in example_ids:\n",
    "            continue\n",
    "        non_dup_examples.append(example)\n",
    "        example_ids.add(example_id)\n",
    "    return non_dup_examples\n",
    "\n",
    "\n",
    "examples = remove_duplication(examples)\n",
    "\n",
    "# random split to train & dev\n",
    "entity_examples = defaultdict(list)\n",
    "for idx, example in enumerate(examples):\n",
    "    entity = example[\"sent\"][example[\"start\"] : example[\"end\"]]\n",
    "    entity_examples[entity].append(idx)\n",
    "\n",
    "all_entity = list(entity_examples.keys())\n",
    "random.seed(1234)\n",
    "random.shuffle(all_entity)\n",
    "\n",
    "# ensure train dev no entity overlap\n",
    "# TODO: better ways to compare entites (e.g. entity id) rather than text\n",
    "train_examples, dev_examples = [], []\n",
    "n_entity = 0\n",
    "while n_entity < len(all_entity) and len(train_examples) < len(examples) * 0.9:\n",
    "    entity = all_entity[n_entity]\n",
    "    train_examples += [examples[idx] for idx in entity_examples[entity]]\n",
    "    n_entity += 1\n",
    "\n",
    "while n_entity < len(all_entity):\n",
    "    entity = all_entity[n_entity]\n",
    "    dev_examples += [examples[idx] for idx in entity_examples[entity]]\n",
    "    n_entity += 1\n",
    "print(f\"#train_examples: {len(train_examples)}, ratio: {len(train_examples) / len(examples)}\")\n",
    "print(f\"#dev_examples: {len(dev_examples)}, ratio: {len(dev_examples) / len(examples)}\")\n",
    "\n",
    "\n",
    "test_examples = remove_duplication(test_examples)\n",
    "# train dataset has no \"/景点/自然风光/池塘\" label\n",
    "test_examples = [example for example in test_examples if \"/景点/自然风光/池塘\" not in example[\"labels\"]]\n",
    "\n",
    "for target_file, split_examples in zip([train_file, dev_file, test_file],\n",
    "                                       [train_examples, dev_examples, test_examples]):\n",
    "    with open(target_file, \"w\") as writer:\n",
    "        json.dump(split_examples, writer, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ratio of entities having > 1 types: 0.0319462568266396\n"
     ]
    }
   ],
   "source": [
    "# train data sentence length distribution\n",
    "import numpy as np\n",
    "\n",
    "def dataset_stat(examples):\n",
    "    print(\"sentence length percentage\")\n",
    "    sent_lens = np.array([len(example[\"sent\"]) for example in examples])\n",
    "    for i in range(90, 101):\n",
    "        print(i, np.percentile(sent_lens, i))\n",
    "\n",
    "    label_set = set()\n",
    "    n_fine_grained = 0\n",
    "    for example in examples:\n",
    "        label_set.update(example[\"labels\"])\n",
    "        if len(example[\"labels\"]) > 1:\n",
    "            n_fine_grained += 1\n",
    "\n",
    "    print(f\"\\n#examples in dataset: {len(examples)}\")\n",
    "    print(f\"#labels in dataset: {len(label_set)}\")\n",
    "    print(f\"#fine grained examples (> 1 labels) ratio: {n_fine_grained / len(examples)}\")\n",
    "\n",
    "    # example type distribution\n",
    "    type_count = Counter()\n",
    "    for example in examples:\n",
    "        type_count.update(example[\"labels\"])\n",
    "    print(\"\\ntype distribution\")\n",
    "    for name, count in type_count.most_common():\n",
    "        print(name, count, f\"{count / len(examples):.5f}\")\n",
    "\n",
    "n_fine_grained_entity = sum(1 if len(types) > 1 else 0 for entity, types in entity_types.items())\n",
    "print(f\"ratio of entities having > 1 types: {n_fine_grained_entity / len(entity_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentence length percentage\n",
      "90 114.0\n",
      "91 119.0\n",
      "92 125.0\n",
      "93 133.0\n",
      "94 143.0\n",
      "95 154.0\n",
      "96 171.0\n",
      "97 195.0\n",
      "98 226.44000000000233\n",
      "99 291.0\n",
      "100 1838.0\n",
      "\n",
      "#examples in dataset: 168679\n",
      "#labels in dataset: 32\n",
      "#fine grained examples (> 1 labels) ratio: 0.018022397571719063\n",
      "\n",
      "type distribution\n",
      "/人物 117127 0.69438\n",
      "/组织机构 42898 0.25432\n",
      "/建筑 7301 0.04328\n",
      "/文物 2063 0.01223\n",
      "/景点 1840 0.01091\n",
      "/景点/场馆 333 0.00197\n",
      "/景点/自然风光 284 0.00168\n",
      "/景点/宗教场所 237 0.00141\n",
      "/景点/自然风光/野外景色 220 0.00130\n",
      "/景点/宗教场所/寺庙 211 0.00125\n",
      "/景点/场馆/文化展馆 185 0.00110\n",
      "/景点/场馆/文化展馆/博物馆 185 0.00110\n",
      "/景点/人文历史景点 142 0.00084\n",
      "/景点/场馆/公园场馆 133 0.00079\n",
      "/景点/场馆/公园场馆/公园 121 0.00072\n",
      "/景点/人文历史景点/红色旅游景区 55 0.00033\n",
      "/景点/人文历史景点/名人故居 44 0.00026\n",
      "/景点/自然风光/山峰 43 0.00025\n",
      "/景点/自然风光/湖泊 42 0.00025\n",
      "/景点/自然风光/池塘 36 0.00021\n",
      "/老字号门店 29 0.00017\n",
      "/景点/人文历史景点/世界文化遗址 27 0.00016\n",
      "/景点/宗教场所/教堂 26 0.00015\n",
      "/景点/场馆/公园场馆/游乐园 19 0.00011\n",
      "/景点/人文历史景点/皇家园林 19 0.00011\n",
      "/景点/自然风光/河流 16 0.00009\n",
      "/景点/自然风光/温泉 15 0.00009\n",
      "/景点/场馆/演出场馆 15 0.00009\n",
      "/景点/场馆/演出场馆/剧院 14 0.00008\n",
      "/景点/场馆/演出场馆/小剧场 9 0.00005\n",
      "/景点/人文历史景点/胡同 6 0.00004\n",
      "/景点/自然风光/雪山 1 0.00001\n"
     ]
    }
   ],
   "source": [
    "dataset_stat(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentence length percentage\n90 109.0\n91 114.0\n92 120.0\n93 126.0\n94 135.0\n95 146.0\n96 160.0\n97 182.11999999999898\n98 216.0799999999981\n99 275.0\n100 1194.0\n\n#examples in dataset: 18697\n#labels in dataset: 23\n#fine grained examples (> 1 labels) ratio: 0.011713109054928597\n\ntype distribution\n/人物 13941 0.74563\n/组织机构 3845 0.20565\n/建筑 874 0.04675\n/景点 142 0.00759\n/文物 83 0.00444\n/景点/自然风光 40 0.00214\n/景点/自然风光/野外景色 32 0.00171\n/景点/宗教场所 21 0.00112\n/景点/宗教场所/寺庙 21 0.00112\n/景点/自然风光/山峰 14 0.00075\n/景点/场馆 7 0.00037\n/老字号门店 5 0.00027\n/景点/人文历史景点 4 0.00021\n/景点/人文历史景点/名人故居 4 0.00021\n/景点/场馆/文化展馆 4 0.00021\n/景点/场馆/文化展馆/博物馆 4 0.00021\n/景点/自然风光/湖泊 3 0.00016\n/景点/场馆/公园场馆 2 0.00011\n/景点/场馆/公园场馆/游乐园 1 0.00005\n/景点/场馆/公园场馆/公园 1 0.00005\n/景点/场馆/演出场馆 1 0.00005\n/景点/场馆/演出场馆/小剧场 1 0.00005\n/景点/场馆/演出场馆/剧院 1 0.00005\n"
     ]
    }
   ],
   "source": [
    "dataset_stat(dev_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentence length percentage\n90 101.0\n91 105.0\n92 107.0\n93 111.0\n94 114.0\n95 118.0\n96 125.0\n97 128.0\n98 133.0\n99 140.14999999999964\n100 150.0\n\n#examples in dataset: 4586\n#labels in dataset: 30\n#fine grained examples (> 1 labels) ratio: 0.2030091583078936\n\ntype distribution\n/人物 1591 0.34693\n/景点 1057 0.23048\n/建筑 996 0.21718\n/组织机构 749 0.16332\n/景点/人文历史景点 568 0.12386\n/文物 544 0.11862\n/景点/场馆 197 0.04296\n/景点/自然风光 143 0.03118\n/景点/场馆/公园场馆 115 0.02508\n/景点/自然风光/野外景色 112 0.02442\n/景点/场馆/公园场馆/公园 110 0.02399\n/景点/宗教场所 98 0.02137\n/景点/宗教场所/寺庙 91 0.01984\n/景点/人文历史景点/世界文化遗址 69 0.01505\n/景点/场馆/文化展馆 53 0.01156\n/景点/场馆/文化展馆/博物馆 46 0.01003\n/景点/场馆/演出场馆/剧院 31 0.00676\n/景点/场馆/演出场馆 31 0.00676\n/景点/自然风光/山峰 26 0.00567\n/景点/人文历史景点/皇家园林 25 0.00545\n/景点/人文历史景点/红色旅游景区 23 0.00502\n/景点/场馆/演出场馆/小剧场 17 0.00371\n/老字号门店 11 0.00240\n/景点/自然风光/湖泊 11 0.00240\n/景点/人文历史景点/名人故居 10 0.00218\n/景点/人文历史景点/胡同 10 0.00218\n/景点/宗教场所/教堂 5 0.00109\n/景点/场馆/公园场馆/游乐园 4 0.00087\n/景点/自然风光/温泉 3 0.00065\n/景点/自然风光/雪山 1 0.00022\n"
     ]
    }
   ],
   "source": [
    "dataset_stat(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}